{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MAKE A \"CREATE PARAMETERS\" SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries\n",
    "# %pip install X\n",
    "# Imports\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUS Pills detection model\n",
    "\n",
    "## Explanation of the imports:\n",
    "\n",
    "| Import name | Usage |\n",
    "| - | - |\n",
    "| pathlib.Path | folder/directory management |\n",
    "| tensorflow | creating AI model |\n",
    "| datetime | creating model name |\n",
    "| json | unpacking parameters from file |\n",
    "| os | rising Tensorflow min log level |\n",
    "\n",
    "## Clearing unnecessary warnings caused by Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbose level\n",
    "VERBOSE = 2\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \".\\\\data\\\\images\\\\.train\"\n",
    "SAVE_DIR = \".\\\\sus\\\\models\"\n",
    "LOG_DIR = \".\\\\sus\\\\.logs\"\n",
    "\n",
    "# Files\n",
    "PARAMETERS_F = \".\\\\sus\\\\parameters\\\\parameters_list.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_model function:\n",
    "\n",
    "Create a package (model, test dataset, train dataset, batch_size, learning rate, epochs). Use the predefined dictionary with desired parameters. Every dictionary should look like that:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"input_y\": <integer>,\n",
    "    \"input_x\": <integer>,\n",
    "    \"batch_size\": <integer>,\n",
    "    \"dimensions\": <integer>,\n",
    "    \"conv_filters\": <list of integers>,\n",
    "    \"dense_units\": <list of integers>,\n",
    "    \"drop_prob\": <list of floats>,\n",
    "    \"learning_rate\": <float>,\n",
    "    \"epochs\": <integer>\n",
    "}\n",
    "```\n",
    "\n",
    "This script already uses a pre-existing *.json file with the list of directories used by the ```map()``` function. You can edit it to create different models. Core features of models are static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_package(params: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load dataset and create the model based on given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\" SET VARIABLES \"\"\"\n",
    "    # Number of pill types we want the model to be albe to predict\n",
    "    outputs = len(list(Path(DATA_DIR).iterdir()))\n",
    "\n",
    "    # Inputs: y = height, x = width\n",
    "    input_y = params['input_y']\n",
    "    input_x = params['input_x']\n",
    "\n",
    "    # Batch size, learning rate, and epochs\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    # Dimensions and color mode\n",
    "    dimensions = params['dimensions']\n",
    "    if dimensions == 1:\n",
    "        color_mode = 'grayscale'\n",
    "    elif dimensions == 3:\n",
    "        color_mode = 'rgb'\n",
    "    \n",
    "    # Convolution filters, dense units, and dropout probability\n",
    "    filters_list = params['conv_filters']\n",
    "    dense_block = zip(params['dense_units'], params['drop_prob'])\n",
    "\n",
    "    # Regularizer used for convolutions\n",
    "    regul_l2 = tf.keras.regularizers.L2()\n",
    "\n",
    "\n",
    "    \"\"\" LOAD DATASET \"\"\"\n",
    "    ds_train, ds_test = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=DATA_DIR,\n",
    "        label_mode=\"int\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=(input_y, input_x),\n",
    "        validation_split=0.2,\n",
    "        seed=123,\n",
    "        subset=\"both\",\n",
    "        color_mode=color_mode,\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\" CREATE MODEL \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Add input layer\n",
    "    model.add(tf.keras.layers.Input((input_y, input_x, dimensions)))\n",
    "\n",
    "    # Add conv2d, maxpool, and batchnorm layers\n",
    "    for filters in filters_list:\n",
    "        model.add(tf.keras.layers.Conv2D(filters, kernel_size=3,\n",
    "                  padding='same', activation='relu', kernel_regularizer=regul_l2))\n",
    "        model.add(tf.keras.layers.MaxPool2D(pool_size=5, padding='same'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # Add flatten layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Add dense and dropout layers\n",
    "    for units, drop_prob in dense_block:\n",
    "        model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=drop_prob))\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(tf.keras.layers.Dense(outputs, activation='relu'))\n",
    "\n",
    "\n",
    "    \"\"\" CREATE A PACKAGE \"\"\"\n",
    "    package = {\n",
    "        'model': model,\n",
    "        'train': ds_train,\n",
    "        'test': ds_test,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'epochs': epochs,\n",
    "    }\n",
    "\n",
    "    return package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the parameters from the file and map all of its contents to create packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PARAMETERS_F, 'r') as f:\n",
    "    parameters_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = list(map(create_package, parameters_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, evaluate, and save the model extracted from every package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for package in packages:\n",
    "\n",
    "    # Unpack all variables\n",
    "    model = package['model']\n",
    "    train = package['train']\n",
    "    test = package['test']\n",
    "    batch_size = package['batch_size']\n",
    "    learning_rate = package['learning_rate']\n",
    "    epochs = package['epochs']\n",
    "\n",
    "    # Print the summary\n",
    "    if (VERBOSE == 2):\n",
    "        model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=[tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)],\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Create the model's name\n",
    "    model_name = f\"cnn-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "    # logging directory\n",
    "    log_dir = f'{LOG_DIR}\\\\{model_name}'\n",
    "\n",
    "    # Initiate TensorBoard\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Fit the dataset to the model\n",
    "    model.fit(train, batch_size=batch_size, epochs=epochs,\n",
    "              verbose=VERBOSE, callbacks=[tensorboard])\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate = model.evaluate(test, batch_size=batch_size,\n",
    "                              verbose=VERBOSE)\n",
    "\n",
    "    # Save the model if plausible\n",
    "    eval_loss = evaluate[0]\n",
    "    eval_accuracy = evaluate[1]\n",
    "    if (eval_loss < .5) and (eval_accuracy > .95):\n",
    "        model.save(f\"{SAVE_DIR}\\\\{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9fe3a0d77b7f701eeb320c3c30fef7c06fc31f1d1fd5594056fae9442a9540c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
